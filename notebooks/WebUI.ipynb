{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Host A Gradio Server on Local GPU\n",
        "Model page: https://huggingface.co/SimulaMet/SoccerChat-qwen2-vl-7b\n"
      ],
      "metadata": {
        "id": "_pCHoZ3fwxxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ms-swift==3.8.0 bitsandbytes decord qwen_vl_utils==0.0.11"
      ],
      "metadata": {
        "id": "_g5R65GdxmEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successful execution below, you get a free link like *https://randomhash.gradio.live* which will open Web UI for you to test and also share publicly. But make sure the server keeps running here. Free colab might be up for 12 hours. Also keep in mind that the random Gradio link is also not permanent."
      ],
      "metadata": {
        "id": "rNDAtbpn9Y_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!FPS_MIN_FRAMES=24 FPS_MAX_FRAMES=24 VIDEO_MAX_PIXELS=100352 swift app \\\n",
        "  --adapters \"SimulaMet/SoccerChat-qwen2-vl-7b\" \\\n",
        "  --model \"Qwen/Qwen2-VL-7B-Instruct\" \\\n",
        "  --use_hf true \\\n",
        "  --attn_impl sdpa \\\n",
        "  --quant_method bnb --quant_bits 4 \\\n",
        "  --bnb_4bit_quant_type nf4 \\\n",
        "  --bnb_4bit_use_double_quant true \\\n",
        "  --bnb_4bit_compute_dtype float16 \\\n",
        "  --max_batch_size 1 \\\n",
        "  --is_multimodal true \\\n",
        "  --studio_title \"SoccerChat\" \\\n",
        "  --stream true \\\n",
        "  --server_name 0.0.0.0 --server_port 7860 --share true --lang en\n",
        "  # quantized for free T4 in Colab; paper reports performance on unquantized model."
      ],
      "metadata": {
        "id": "cKmSLgG2xXps",
        "outputId": "40062ebd-2d25-4194-f80f-a7f34616432e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run sh: `/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/swift/cli/app.py --adapters SimulaMet/SoccerChat-qwen2-vl-7b --model Qwen/Qwen2-VL-7B-Instruct --use_hf true --attn_impl sdpa --quant_method bnb --quant_bits 4 --bnb_4bit_quant_type nf4 --bnb_4bit_use_double_quant true --bnb_4bit_compute_dtype float16 --max_batch_size 1 --is_multimodal true --studio_title SoccerChat --stream true --server_name 0.0.0.0 --server_port 7860 --share true --lang en`\n",
            "2025-09-16 10:03:47.641700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758017027.662981    7561 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758017027.669389    7561 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758017027.685425    7561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017027.685463    7561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017027.685468    7561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017027.685471    7561 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-16 10:03:47.690411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO:swift] Successfully registered `/usr/local/lib/python3.12/dist-packages/swift/llm/dataset/data/dataset_info.json`.\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: SimulaMet/SoccerChat-qwen2-vl-7b\n",
            "Fetching 13 files: 100% 13/13 [00:00<00:00, 162279.62it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--SimulaMet--SoccerChat-qwen2-vl-7b/snapshots/e9d194df879f52c64bfb03e43cabdc6a3ecbecc8\n",
            "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: Qwen/Qwen2-VL-7B-Instruct\n",
            "Fetching 12 files: 100% 12/12 [00:00<00:00, 28532.68it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/eed13092ef92e448dd6875b2a00151bd3f7db0ac\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
            "[INFO:swift] Setting args.lazy_tokenize: True\n",
            "[INFO:swift] args.result_path: /content/result/Qwen2-VL-7B-Instruct/deploy_result/20250916-100355.jsonl\n",
            "[INFO:swift] Global seed set to 42\n",
            "[INFO:swift] args: AppArguments(model='Qwen/Qwen2-VL-7B-Instruct', model_type='qwen2_vl', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl='sdpa', new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen2_vl', system='You are a helpful assistant.', max_length=32768, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_free=False, padding_side='right', loss_scale='default', sequence_parallel_size=1, response_prefix=None, template_backend='swift', dataset=[], val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=True, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method='bnb', quant_bits=4, hqq_axis=None, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=True, stop_words=[], logprobs=False, top_logprobs=None, ckpt_dir=None, lora_modules=[], tuner_backend='peft', train_type='lora', adapters=['/root/.cache/huggingface/hub/models--SimulaMet--SoccerChat-qwen2-vl-7b/snapshots/e9d194df879f52c64bfb03e43cabdc6a3ecbecc8'], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, lazy_tokenize=True, cached_dataset=[], custom_register_path=[], use_hf=True, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt={}, vllm_max_lora_rank=16, vllm_enable_prefix_caching=False, vllm_use_async_engine=True, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_data_parallel_size=1, gpu_memory_utilization=None, tensor_parallel_size=None, limit_mm_per_prompt=None, data_parallel_size=None, use_async_engine=None, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/content/result/Qwen2-VL-7B-Instruct/deploy_result/20250916-100355.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, host='0.0.0.0', port=8000, api_key=None, ssl_keyfile=None, ssl_certfile=None, owned_by='swift', served_model_name=None, verbose=False, log_interval=20, log_level='info', max_logprobs=20, server_name='0.0.0.0', server_port=7860, share=True, lang='en', base_url=None, studio_title='SoccerChat', is_multimodal=True)\n",
            "[INFO:swift] Start time of running main: 2025-09-16 10:03:55.309695\n",
            "[INFO:swift] swift.__version__: 3.8.1\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--SimulaMet--SoccerChat-qwen2-vl-7b/snapshots/e9d194df879f52c64bfb03e43cabdc6a3ecbecc8\n",
            "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: Qwen/Qwen2-VL-7B-Instruct\n",
            "Fetching 12 files: 100% 12/12 [00:00<00:00, 32243.21it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/eed13092ef92e448dd6875b2a00151bd3f7db0ac\n",
            "2025-09-16 10:04:04.698425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758017044.718273    7676 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758017044.724606    7676 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758017044.740246    7676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017044.740274    7676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017044.740277    7676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758017044.740283    7676 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[INFO:swift] Successfully registered `/usr/local/lib/python3.12/dist-packages/swift/llm/dataset/data/dataset_info.json`.\n",
            "[INFO:swift] Global seed set to 42\n",
            "[INFO:swift] args: DeployArguments(model='Qwen/Qwen2-VL-7B-Instruct', model_type='qwen2_vl', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl='sdpa', new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen2_vl', system='You are a helpful assistant.', max_length=32768, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_free=False, padding_side='right', loss_scale='default', sequence_parallel_size=1, response_prefix=None, template_backend='swift', dataset=[], val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=True, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method='bnb', quant_bits=4, hqq_axis=None, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=True, stop_words=[], logprobs=False, top_logprobs=None, ckpt_dir=None, lora_modules=[], tuner_backend='peft', train_type='lora', adapters=['/root/.cache/huggingface/hub/models--SimulaMet--SoccerChat-qwen2-vl-7b/snapshots/e9d194df879f52c64bfb03e43cabdc6a3ecbecc8'], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, lazy_tokenize=True, cached_dataset=[], custom_register_path=[], use_hf=True, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt={}, vllm_max_lora_rank=16, vllm_enable_prefix_caching=False, vllm_use_async_engine=True, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_data_parallel_size=1, gpu_memory_utilization=None, tensor_parallel_size=None, limit_mm_per_prompt=None, data_parallel_size=None, use_async_engine=None, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/content/result/Qwen2-VL-7B-Instruct/deploy_result/20250916-100355.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, host='0.0.0.0', port=8000, api_key=None, ssl_keyfile=None, ssl_certfile=None, owned_by='swift', served_model_name=None, verbose=False, log_interval=20, log_level='info', max_logprobs=20)\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: Qwen/Qwen2-VL-7B-Instruct\n",
            "Fetching 17 files: 100% 17/17 [00:00<00:00, 173910.17it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/eed13092ef92e448dd6875b2a00151bd3f7db0ac\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
            "[INFO:swift] attn_impl: sdpa\n",
            "[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'quantization_config': BitsAndBytesConfig {\n",
            "  \"_load_in_4bit\": true,\n",
            "  \"_load_in_8bit\": false,\n",
            "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": true,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": [\n",
            "    \"model.visual\",\n",
            "    \"model.visual.merger\",\n",
            "    \"lm_head\"\n",
            "  ],\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            "}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 5/5 [01:29<00:00, 17.95s/it]\n",
            "[INFO:swift] Setting image_factor: 28. You can adjust this hyperparameter through the environment variable: `IMAGE_FACTOR`.\n",
            "[INFO:swift] Setting min_pixels: 3136. You can adjust this hyperparameter through the environment variable: `MIN_PIXELS`.\n",
            "[INFO:swift] Setting max_pixels: 12845056. You can adjust this hyperparameter through the environment variable: `MAX_PIXELS`.\n",
            "[INFO:swift] Setting max_ratio: 200. You can adjust this hyperparameter through the environment variable: `MAX_RATIO`.\n",
            "[INFO:swift] Setting video_min_pixels: 100352. You can adjust this hyperparameter through the environment variable: `VIDEO_MIN_PIXELS`.\n",
            "[INFO:swift] Using environment variable `VIDEO_MAX_PIXELS`, Setting video_max_pixels: 100352.\n",
            "[INFO:swift] Using environment variable `VIDEO_TOTAL_PIXELS`, Setting video_total_pixels: 90316800.\n",
            "[INFO:swift] Setting frame_factor: 2. You can adjust this hyperparameter through the environment variable: `FRAME_FACTOR`.\n",
            "[INFO:swift] Setting fps: 2.0. You can adjust this hyperparameter through the environment variable: `FPS`.\n",
            "[INFO:swift] Using environment variable `FPS_MIN_FRAMES`, Setting fps_min_frames: 24.\n",
            "[INFO:swift] Using environment variable `FPS_MAX_FRAMES`, Setting fps_max_frames: 24.\n",
            "[INFO:swift] default_system: 'You are a helpful assistant.'\n",
            "[INFO:swift] max_length: 32768\n",
            "[INFO:swift] response_prefix: ''\n",
            "[INFO:swift] agent_template: hermes\n",
            "[INFO:swift] norm_bbox: norm1000\n",
            "[INFO:swift] Setting ROOT_IMAGE_DIR: None. You can adjust this hyperparameter through the environment variable: `ROOT_IMAGE_DIR`.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.visual.blocks.0.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.0.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.0.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.0.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.0.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.0.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.0.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.0.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.1.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.1.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.1.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.1.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.1.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.1.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.1.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.1.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.2.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.2.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.2.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.2.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.2.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.2.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.2.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.2.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.3.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.3.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.3.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.3.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.3.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.3.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.3.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.3.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.4.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.4.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.4.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.4.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.4.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.4.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.4.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.4.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.5.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.5.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.5.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.5.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.5.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.5.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.5.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.5.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.6.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.6.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.6.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.6.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.6.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.6.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.6.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.6.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.7.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.7.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.7.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.7.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.7.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.7.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.7.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.7.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.8.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.8.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.8.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.8.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.8.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.8.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.8.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.8.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.9.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.9.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.9.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.9.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.9.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.9.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.9.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.9.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.10.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.10.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.10.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.10.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.10.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.10.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.10.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.10.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.11.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.11.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.11.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.11.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.11.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.11.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.11.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.11.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.12.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.12.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.12.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.12.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.12.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.12.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.12.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.12.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.13.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.13.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.13.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.13.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.13.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.13.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.13.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.13.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.14.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.14.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.14.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.14.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.14.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.14.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.14.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.14.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.15.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.15.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.15.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.15.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.15.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.15.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.15.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.15.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.16.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.16.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.16.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.16.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.16.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.16.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.16.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.16.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.17.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.17.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.17.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.17.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.17.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.17.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.17.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.17.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.18.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.18.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.18.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.18.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.18.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.18.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.18.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.18.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.19.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.19.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.19.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.19.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.19.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.19.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.19.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.19.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.20.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.20.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.20.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.20.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.20.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.20.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.20.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.20.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.21.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.21.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.21.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.21.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.21.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.21.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.21.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.21.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.22.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.22.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.22.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.22.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.22.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.22.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.22.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.22.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.23.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.23.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.23.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.23.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.23.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.23.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.23.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.23.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.24.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.24.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.24.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.24.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.24.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.24.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.24.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.24.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.25.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.25.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.25.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.25.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.25.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.25.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.25.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.25.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.26.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.26.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.26.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.26.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.26.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.26.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.26.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.26.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.27.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.27.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.27.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.27.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.27.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.27.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.27.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.27.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.28.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.28.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.28.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.28.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.28.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.28.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.28.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.28.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.29.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.29.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.29.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.29.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.29.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.29.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.29.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.29.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.30.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.30.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.30.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.30.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.30.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.30.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.30.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.30.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.blocks.31.attn.qkv.lora_A.default.weight', 'base_model.model.model.visual.blocks.31.attn.qkv.lora_B.default.weight', 'base_model.model.model.visual.blocks.31.attn.proj.lora_A.default.weight', 'base_model.model.model.visual.blocks.31.attn.proj.lora_B.default.weight', 'base_model.model.model.visual.blocks.31.mlp.fc1.lora_A.default.weight', 'base_model.model.model.visual.blocks.31.mlp.fc1.lora_B.default.weight', 'base_model.model.model.visual.blocks.31.mlp.fc2.lora_A.default.weight', 'base_model.model.model.visual.blocks.31.mlp.fc2.lora_B.default.weight', 'base_model.model.model.visual.merger.mlp.0.lora_A.default.weight', 'base_model.model.model.visual.merger.mlp.0.lora_B.default.weight', 'base_model.model.model.visual.merger.mlp.2.lora_A.default.weight', 'base_model.model.model.visual.merger.mlp.2.lora_B.default.weight'].\n",
            "  warnings.warn(warn_message)\n",
            "[INFO:swift] model: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Qwen2VLForConditionalGeneration(\n",
            "      (model): Qwen2VLModel(\n",
            "        (visual): Qwen2VisionTransformerPretrainedModel(\n",
            "          (patch_embed): PatchEmbed(\n",
            "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
            "          )\n",
            "          (rotary_pos_emb): VisionRotaryEmbedding()\n",
            "          (blocks): ModuleList(\n",
            "            (0-31): 32 x Qwen2VLVisionBlock(\n",
            "              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): VisionAttention(\n",
            "                (qkv): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3840, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (proj): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "              (mlp): VisionMlp(\n",
            "                (fc1): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=5120, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (act): QuickGELUActivation()\n",
            "                (fc2): lora.Linear(\n",
            "                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=5120, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (merger): PatchMerger(\n",
            "            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
            "            (mlp): Sequential(\n",
            "              (0): lora.Linear(\n",
            "                (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=5120, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (1): GELU(approximate='none')\n",
            "              (2): lora.Linear(\n",
            "                (base_layer): Linear(in_features=5120, out_features=3584, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=3584, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (language_model): Qwen2VLTextModel(\n",
            "          (embed_tokens): Embedding(152064, 3584)\n",
            "          (layers): ModuleList(\n",
            "            (0-27): 28 x Qwen2VLDecoderLayer(\n",
            "              (self_attn): Qwen2VLAttention(\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3584, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=512, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=512, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3584, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (rotary_emb): Qwen2VLRotaryEmbedding()\n",
            "              )\n",
            "              (mlp): Qwen2MLP(\n",
            "                (gate_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=18944, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (up_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=3584, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=18944, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (down_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=18944, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=3584, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (act_fn): SiLU()\n",
            "              )\n",
            "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
            "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
            "            )\n",
            "          )\n",
            "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
            "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
            "        )\n",
            "      )\n",
            "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[INFO:swift] Start time of running main: 2025-09-16 10:05:43.115564\n",
            "[INFO:swift] swift.__version__: 3.8.1\n",
            "[INFO:swift] model_list: ['Qwen2-VL-7B-Instruct']\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m7676\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     127.0.0.1:50860 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/swift/llm/app/build_ui.py:116: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label='Chatbot')\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://419148335feb433373.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.06100769, 'samples/s': 0.0, 'tokens/s': 0.0}\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.01974642, 'samples/s': 0.0, 'tokens/s': 0.0}\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.01869166, 'samples/s': 0.0, 'tokens/s': 0.0}\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.02010682, 'samples/s': 0.0, 'tokens/s': 0.0}\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.02013908, 'samples/s': 0.0, 'tokens/s': 0.0}\n",
            "[INFO:swift] {'num_prompt_tokens': 0, 'num_generated_tokens': 0, 'num_samples': 0, 'runtime': 20.02020521, 'samples/s': 0.0, 'tokens/s': 0.0}\n"
          ]
        }
      ]
    }
  ]
}
